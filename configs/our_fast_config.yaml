
# restore_model: ''
restore_model: ./pretrained/fastpitch_ar_adv.pth
# restore_model: ./checkpoints/exp_fp_adv/states.pth

log_dir: logs/exp_fp_adv
checkpoint_dir: checkpoints/exp_fp_adv

# dataset
train_wavs_path: /content/tts-arabic-pytorch/Mini Arabic Youtube Videos/new_wavs
train_labels: /content/tts-arabic-pytorch/Mini Arabic Youtube Videos/final_text_files/train_phon.txt

test_wavs_path: /content/tts-arabic-pytorch/Mini Arabic Youtube Videos/new_wavs_test
test_labels: /content/tts-arabic-pytorch/Mini Arabic Youtube Videos/final_text_files/test_phon.txt

label_pattern: '"(?P<filename>.*)" "(?P<phonemes>.*)"'
# label_pattern: (?P<arabic>.*)\|(?P<filestem>.*)

# for fastpitch
f0_dict_path: /content/tts-arabic-pytorch/Mini Arabic Youtube Videos/final_text_files/pitch_dict.pt

f0_mean: 250.72385326088502
f0_std: 263.17980630869454

# loss weights
gan_loss_weight: 3.
feat_loss_weight: 1.

# batch sizes
max_lengths: [1000, 1300, 1850, 30000] # 1 frame â‰ˆ 11.6ms
batch_sizes: [10, 8, 6, 4]

# optimizers
g_lr: 1.0e-4    # lr for AdamW optimizer (generator)
g_beta1: 0.     # beta1 for AdamW optimizer (generator)
g_beta2: 0.99   # beta2 for AdamW optimizer (generator)

d_lr: 1.0e-4    # lr for AdamW optimizer (discriminator)
d_beta1: 0.     # beta1 for AdamW optimizer (discriminator)
d_beta2: 0.99   # beta2 for AdamW optimizer (discriminator)

n_save_states_iter: 100
n_save_backup_iter: 1000

epochs: 12
decoder_max_step: 3000

random_seed: False

batch_size: 8
learning_rate: 1.0e-3
weight_decay: 1.0e-6
grad_clip_thresh: 1.0

cache_dataset: True
use_cuda_if_available: True

balanced_sampling: False

# vocoder
vocoder_state_path: pretrained/hifigan-asc-v1/hifigan-asc.pth
vocoder_config_path: pretrained/hifigan-asc-v1/config.json

# diacritizers
shakkala_path: pretrained/diacritizers/shakkala_second_model6.pth
shakkelha_path: pretrained/diacritizers/shakkelha_rnn_3_big_20.pth
